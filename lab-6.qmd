---
title: “Lab 6“
author: Rachel Delorie
format: html
execute:
  echo: true
---
```{r setup}
# setup
library(tidymodels)
library(recipes)
library(yardstick)
library(ggthemes)
library(ggplot2)
library(patchwork)
library(parsnip)
library(tidyverse)
library(powerjoin)
library(xgboost)
library(purrr)
library(glue)
library(vip)
library(baguette)

root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```

```{r aridity map}
map_aridity <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
    labs(color = "Aridity", title = "Aridity") +
   geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "blue", high = "red") +
  ggthemes::theme_map()

map_pmean <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
   geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  labs(color = "q_mean", title = "Mean Daily Precipitation") +
  ggthemes::theme_map() 
  
combined_maps <- map_aridity + map_pmean + plot_layout(ncol = 2)
combined_maps
```

```{r question 3}
# Define model
# Create a recipe to preprocess the data
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)

lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients


rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)

# adding an nnet to provided code
nn_model <- bag_mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression") 

nn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(nn_model) %>% 
  fit(data = camels_train)

# adding an xgboost to provided code
xg_model <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") 

xg_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(xg_model) %>% 
  fit(data = camels_train)

wf <- workflow_set(list(rec), list(lm_model, rf_model, nn_model, xg_model)) %>%
  workflow_map(resamples = camels_cv) 

autoplot(wf)

# Out of the 4 models above, I would move forward with using the rand_forest model because it has a low rmse (~.56) and a decently high rsq (~.75). 
```

```{r build your own}
#> Build Your Own: predict mean streamflow using CAMELS
set.seed(123)
# Generate split
camels_splitting <- initial_split(camels, prop = 0.75)
camels_training <- training(camels_split)
camels_testing  <- testing(camels_split)
camels_10cv <- vfold_cv(camels_training, v = 10)

# make a recipe
recipe <- recipe(logQmean ~ pet_mean + p_mean, data = camels_training) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ pet_mean:p_mean) %>%  
  step_naomit(all_predictors(), all_outcomes())

baked_data <- prep(recipe, camels_training) %>%  
  bake(new_data = NULL)
# I chose to use pet_mean and p_mean to measure mean streamflow because they are measurements of evapotranspiration and precipitation so they can be used to guage how much water may be in the stream.

#define rf model
rf_camel_model <-rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

#2 others of my choice
xg_camel_model <-boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

lm_camel_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

#setup workflow
wf <- workflow_set(list(rec),list(rf_camel_model,xg_camel_model,lm_camel_model)) %>% 
  workflow_map('fit_resamples', resamples = camels_10cv)

autoplot(wf)
rank_results(wf, rank_metric = "rsq", select_best = TRUE)

# I think that the random forest model is still the best because it has the lowest rmse, and a high r squared value of ~.77. 

#extract
rf_fit = workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(rf_camel_model) %>% 
  fit(data = camels_training)

rf_camel_data <- augment(rf_fit, new_data = camels_testing)
dim(rf_data)

ggplot(rf_camel_data, aes(x = logQmean, y = .pred, colour = pet_mean)) +
  labs(title = "Observed vs. Predicted Values", x = "Log Mean", y = "Predicted") +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

# I think the graph shows a decent fit of points to the black line, meaning that this model predicts logQmean values pretty well. Some values deviate, and those could be errors or underpredictions. Additionally, this model seems to do better at predicting higher pet_mean values because those values deviate from the black line much less than logQmean values between -4 and 0. 

```