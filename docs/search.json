[
  {
    "objectID": "lab-6.html",
    "href": "lab-6.html",
    "title": "“Lab 6“",
    "section": "",
    "text": "# setup\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(ggthemes)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(parsnip)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(powerjoin)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(purrr)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\nmap_aridity &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n    labs(color = \"Aridity\", title = \"Aridity\") +\n   geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  ggthemes::theme_map()\n\nmap_pmean &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n   geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(color = \"q_mean\", title = \"Mean Daily Precipitation\") +\n  ggthemes::theme_map() \n  \ncombined_maps &lt;- map_aridity + map_pmean + plot_layout(ncol = 2)\ncombined_maps\n\n\n\n\n\n\n\n\n\n# Define model\n# Create a recipe to preprocess the data\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  \n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# adding an nnet to provided code\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\") \n\nnn_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;% \n  add_model(nn_model) %&gt;% \n  fit(data = camels_train)\n\n# adding an xgboost to provided code\nxg_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\") \n\nxg_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;% \n  add_model(xg_model) %&gt;% \n  fit(data = camels_train)\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, nn_model, xg_model)) %&gt;%\n  workflow_map(resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n# Out of the 4 models above, I would move forward with using the rand_forest model because it has a low rmse (~.56) and a decently high rsq (~.75). \n\n\n#&gt; Build Your Own: predict mean streamflow using CAMELS\nset.seed(123)\n# Generate split\ncamels_splitting &lt;- initial_split(camels, prop = 0.75)\ncamels_training &lt;- training(camels_split)\ncamels_testing  &lt;- testing(camels_split)\ncamels_10cv &lt;- vfold_cv(camels_training, v = 10)\n\n# make a recipe\nrecipe &lt;- recipe(logQmean ~ pet_mean + p_mean, data = camels_training) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ pet_mean:p_mean) %&gt;%  \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(recipe, camels_training) %&gt;%  \n  bake(new_data = NULL)\n# I chose to use pet_mean and p_mean to measure mean streamflow because they are measurements of evapotranspiration and precipitation so they can be used to guage how much water may be in the stream.\n\n#define rf model\nrf_camel_model &lt;-rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\n#2 others of my choice\nxg_camel_model &lt;-boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\nlm_camel_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n#setup workflow\nwf &lt;- workflow_set(list(rec),list(rf_camel_model,xg_camel_model,lm_camel_model)) %&gt;% \n  workflow_map('fit_resamples', resamples = camels_10cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.560  0.0433    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.779  0.0250    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.560  0.0433    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.773  0.0212    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.586  0.0398    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.761  0.0195    10 recipe       boos…     3\n\n# I think that the random forest model is still the best because it has the lowest rmse, and a high r squared value of ~.77. \n\n#extract\nrf_fit = workflow() %&gt;% \n  add_recipe(recipe) %&gt;% \n  add_model(rf_camel_model) %&gt;% \n  fit(data = camels_training)\n\nrf_camel_data &lt;- augment(rf_fit, new_data = camels_testing)\ndim(rf_data)\n\n[1] 135  60\n\nggplot(rf_camel_data, aes(x = logQmean, y = .pred, colour = pet_mean)) +\n  labs(title = \"Observed vs. Predicted Values\", x = \"Log Mean\", y = \"Predicted\") +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# I think the graph shows a decent fit of points to the black line, meaning that this model predicts logQmean values pretty well. Some values deviate, and those could be errors or underpredictions. Additionally, this model seems to do better at predicting higher pet_mean values because those values deviate from the black line much less than logQmean values between -4 and 0."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(ggthemes)\nlibrary(ggplot2)\nlibrary(workflowsets)\nlibrary(patchwork)\nlibrary(ggfortify)\n\nRegistered S3 method overwritten by 'ggfortify':\n  method          from   \n  autoplot.glmnet parsnip\n\nlibrary(parsnip)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\nlibrary(powerjoin)\nlibrary(skimr)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\n# Data Import/Tidy/Transform    \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id') \n\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean)) %&gt;% \n  mutate(across(everything(), as.double))\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(everything(), as.double)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\nskim(camels)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n59\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n59\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngauge_id\n0\n1.00\n6265830.84\n3976867.52\n1013500.00\n2370650.00\n6278300.00\n9382765.00\n14400000.00\n▇▃▅▃▃\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nhigh_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nlow_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ngeol_1st_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\ngeol_2nd_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\ndom_land_cover\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nlogQmean\n1\n1.00\n-0.11\n1.17\n-5.39\n-0.46\n0.12\n0.56\n2.27\n▁▁▂▇▂\n\n\n\n\nvis_dat(camels)\n\n\n\n\n\n\n\n\n\nset.seed(567)\n\ncamels &lt;- camels %&gt;%  \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n# Cross-validation folds\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Recipe\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n# Define models\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\") \n\nxg_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\") \n\n# Combine into a workflow set\nwf &lt;- workflow_set(\n  preproc = list(rec),\n  models = list(\n    lm_model = lm_model,\n    rf_model = rf_model,\n    nn_model = nn_model,\n    xg_model = xg_model\n  )) %&gt;%\n  workflow_map(resamples = camels_cv)\n\n# Plot\nautoplot(wf)\n\n\n\n\n\n\n\n\n\n#model tuning\nnn_model &lt;- bag_mlp(\n  hidden_units = tune(), \n  penalty = tune()\n) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf_tune&lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;% \n  add_model(nn_model)\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\n\n# define search space\nmy.grid &lt;- grid_space_filling(dials, size = 20)\n\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\nautoplot(model_params)\n\n\n\n\n\n\n\ncollect_metrics(model_params)\n\n# A tibble: 60 × 8\n   hidden_units       penalty .metric .estimator  mean     n std_err .config    \n          &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1            1 0.000000144   mae     standard   0.395    10  0.0219 Preprocess…\n 2            1 0.000000144   rmse    standard   0.584    10  0.0378 Preprocess…\n 3            1 0.000000144   rsq     standard   0.742    10  0.0188 Preprocess…\n 4            1 0.0000616     mae     standard   0.394    10  0.0224 Preprocess…\n 5            1 0.0000616     rmse    standard   0.587    10  0.0404 Preprocess…\n 6            1 0.0000616     rsq     standard   0.741    10  0.0203 Preprocess…\n 7            2 0.0264        mae     standard   0.344    10  0.0192 Preprocess…\n 8            2 0.0264        rmse    standard   0.557    10  0.0361 Preprocess…\n 9            2 0.0264        rsq     standard   0.760    10  0.0213 Preprocess…\n10            2 0.00000000113 mae     standard   0.347    10  0.0176 Preprocess…\n# ℹ 50 more rows\n\nbest_mae &lt;- show_best(model_params, metric = \"mae\", n = 1)\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n#&gt; The first row shows the mean MAE across resamples. It also showws the standard error of the MAE estimate, the number of resamples used, and the mean standard error. Penalty is the best hyperparameter set for this model. \n\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)\nfinal_fit &lt;- last_fit(final_wf, split = camels_split)\nlast_fit &lt;- last_fit(final_wf, split = camels_split)\nfinal_metrics &lt;- collect_metrics(last_fit)\n\n# the final model's rmse 53.5% and the rsq is 78%. This means that 78% of the variance is explained by the model. This is a decent number. The rmse is the average prediction error, and this percentage is above 50% which is a pretty high error amount. This model is reasonably good, but the rmse suggests plenty of room for improvement. \n\npredictions &lt;- collect_predictions(model_params)\n\nggplot(predictions, aes(x = .pred, y = logQmean)) +\n  geom_smooth(method = lm, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  scale_color_gradient() +\n  labs(\n    title = \"Actual vs. Predicted Values\", \n    x = \"Predicted\", \n    y = \"Actual\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfinal_fit_full &lt;- fit(final_wf, data = camels)\naugmented_preds &lt;- augment(final_fit_full, new_data = camels)\n\naugmented_preds &lt;- augmented_preds %&gt;% \n  mutate(residual_sq = (logQmean - .pred)^2)\n\nmap_preds &lt;- ggplot(augmented_preds, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +\n  scale_color_viridis_c(name = \"Predicted\") +\n  coord_fixed() +\n  labs(title = \"Map of Predicted logQmean\") +\n  theme_minimal()\n\nmap_resid &lt;- ggplot(augmented_preds, aes(x = .pred, y = residual_sq)) +\n  geom_point() +\n  coord_fixed() +\n   scale_color_viridis_c(name = \"Residual²\") +\n  labs(title = \"Map of Squared Residuals\") +\n  theme_minimal()\n\nmaps_combined &lt;- map_preds + map_resid\n\nmaps_combined\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  }
]