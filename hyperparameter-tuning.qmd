---
title: "hyperparameter-tuning"
author: Rachel Delorie
format: html
execute:
  echo: true
---
```{r setup/EDA}
library(tidymodels)
library(recipes)
library(yardstick)
library(ggthemes)
library(ggplot2)
library(workflowsets)
library(patchwork)
library(ggfortify)
library(parsnip)
library(tidyverse)
library(visdat)
library(powerjoin)
library(skimr)
library(xgboost)
library(dplyr)
library(purrr)
library(patchwork)
library(glue)
library(vip)
library(baguette)

# Data Import/Tidy/Transform	
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...

local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id') 

camels <- camels %>% 
  mutate(logQmean = log(q_mean)) %>% 
  mutate(across(everything(), as.double))

skim(camels)
vis_dat(camels)

```

```{r}
set.seed(567)

camels <- camels %>%  
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

# Cross-validation folds
camels_cv <- vfold_cv(camels_train, v = 10)

# Recipe
rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) %>% 
  step_naomit(all_predictors(), all_outcomes())

# Define models
lm_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

rf_model <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

nn_model <- bag_mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression") 

xg_model <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") 

# Combine into a workflow set
wf <- workflow_set(
  preproc = list(rec),
  models = list(
    lm_model = lm_model,
    rf_model = rf_model,
    nn_model = nn_model,
    xg_model = xg_model
  )) %>%
  workflow_map(resamples = camels_cv)

# Plot
autoplot(wf)
```

```{r Lab 8}
#model tuning
nn_model <- bag_mlp(
  hidden_units = tune(), 
  penalty = tune()
) %>%
  set_engine("nnet") %>%
  set_mode("regression")

wf_tune<- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(nn_model)

dials <- extract_parameter_set_dials(wf_tune)

# define search space
my.grid <- grid_space_filling(dials, size = 20)

model_params <-  tune_grid(
    wf_tune,
    resamples = camels_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )
autoplot(model_params)

collect_metrics(model_params)
best_mae <- show_best(model_params, metric = "mae", n = 1)
hp_best <- select_best(model_params, metric = "mae")
#> The first row shows the mean MAE across resamples. It also showws the standard error of the MAE estimate, the number of resamples used, and the mean standard error. Penalty is the best hyperparameter set for this model. 

final_wf <- finalize_workflow(wf_tune, hp_best)
final_fit <- last_fit(final_wf, split = camels_split)
last_fit <- last_fit(final_wf, split = camels_split)
final_metrics <- collect_metrics(last_fit)

# the final model's rmse 53.5% and the rsq is 78%. This means that 78% of the variance is explained by the model. This is a decent number. The rmse is the average prediction error, and this percentage is above 50% which is a pretty high error amount. This model is reasonably good, but the rmse suggests plenty of room for improvement. 

predictions <- collect_predictions(model_params)

ggplot(predictions, aes(x = .pred, y = logQmean)) +
  geom_smooth(method = lm, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_gradient() +
  labs(
    title = "Actual vs. Predicted Values", 
    x = "Predicted", 
    y = "Actual")

final_fit_full <- fit(final_wf, data = camels)
augmented_preds <- augment(final_fit_full, new_data = camels)

augmented_preds <- augmented_preds %>% 
  mutate(residual_sq = (logQmean - .pred)^2)

map_preds <- ggplot(augmented_preds, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +
  scale_color_viridis_c(name = "Predicted") +
  coord_fixed() +
  labs(title = "Map of Predicted logQmean") +
  theme_minimal()

map_resid <- ggplot(augmented_preds, aes(x = .pred, y = residual_sq)) +
  geom_point() +
  coord_fixed() +
   scale_color_viridis_c(name = "ResidualÂ²") +
  labs(title = "Map of Squared Residuals") +
  theme_minimal()

maps_combined <- map_preds + map_resid

maps_combined

```

